---
title: "Mixtures of multivariate Gaussian distributions"
author: "Alessandra Cabassi"
date: "`r Sys.Date()`"
output: 
    html_document:
      fig_width: 8
      theme: united
      toc: yes
vignette: >
  %\VignetteIndexEntry{Mixtures of multivariate Gaussian distributions}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Mixtures of multivariate Gaussians

Suppose we have a small dataset containing two clusters. 

```{r example1, message=FALSE, warning=FALSE, cache=TRUE}
library(mvtnorm)
library(vimix)

## Load a dataset containing 200 2-dimensional data points
data <- rbind(rmvnorm(100,c(-3,0)), rmvnorm(100,c(3,0)))
```

Then we can use the function *vimix* to fit a mixture of multivariate Gaussian distributions:

```{r example1-vimix, message=FALSE, warning=FALSE, cache=TRUE}
## Use variational inference for mixture of Gaussians to find clusters
output <- vimix(data, K = 7, verbose = T)
```

Let's see what our clusters look like:

```{r example1-plot, fig.show='hold', message=FALSE, warning=FALSE, cache=TRUE}
## Plot cluster labels
library(ggplot2)
library(broom)
library(gridExtra)

## Convert data matrix and cluster labels to data.frame
df <- tidy(data)
df$label <- as.factor(output$label)

## Plot clusters
ggplot(df, aes(X1, X2, col = label)) + geom_point()
```

The lower bound increases at each iteration of the algorithm and can be used to check whether the algorithm has converged. Another interesting value to check at each iteration is the number of non-empty clusters.

```{r example1-lb, fig.show='hold', message=FALSE, warning=FALSE, cache=TRUE}
## Check that the lower bound is monotonically increasing

lb <- tidy(output$L[-1])
lb$ELBO <- lb$x
lb$x <- NULL
lb$iter <- c(1:length(output$L[-1]))
lb$number_clusters <- output$Cl[-1]

## Plot lower bound
plot_lb <- ggplot(lb, aes(x=iter,y=ELBO)) + geom_line(linetype = "dashed") + geom_point()

## Plot number of non-empty clusters
plot_nc <- ggplot(lb, aes(x=iter,y=number_clusters)) + geom_line(linetype = "dashed") + geom_point()

grid.arrange(plot_lb, plot_nc, ncol = 2)
```

Finally, it is important to note that the algorithm doesn't always converge to the same solution. Depending on the initialization, it will converge to different local optima. However, by runnin the algorithm multiple times, we can see that, the optimal solution is almost always reached, in this simple case. 

```{r example1-figure10-7, fig.show='hold', message=FALSE, warning = FALSE, cache=TRUE}
maxK <- 10
n_random_starts <- 100
ELBO <- matrix(0, maxK-1, n_random_starts)
for(k in 2:maxK){
    for(j in 1:n_random_starts){
        output <- vimix(data, K = k)
        ELBO[k-1,j] <- output$L[length(output$L)]
    }
}

library(reshape)
ELBO <- melt(t(ELBO))
names(ELBO) <- c('start_n', 'K', 'ELBO')
ELBO$K <- ELBO$K + 1

ggplot(ELBO, aes(x = K, y = ELBO)) + geom_point() + geom_jitter()
```

Now, let's try to use the same mixture model as above, but with the additional assumption that all the variables are independent.

```{r example1-ind, message=FALSE, warning=FALSE, cache=TRUE}

## Use variational inference for mixture of Gaussians to find clusters
output <- vimix(data, K = 7, indep = T, verbose = T)
```

We can again check the clustering solution:

```{r example1-ind-plot, fig.show='hold', message=FALSE, warning=FALSE, cache=TRUE}
## Convert data matrix and cluster labels to data.frame
df <- tidy(data)
df$label <- as.factor(output$label)

## Plot clusters
ggplot(df, aes(X1, X2, col = label)) + geom_point()
```

and the lower bound and number of non-empty clusters at each iteration:

```{r example1-ind-lb, fig.show='hold', message=FALSE, warning=FALSE, cache=TRUE}
## Check that the lower bound is monotonically increasing

lb <- tidy(output$L[-1])
lb$ELBO <- lb$x
lb$x <- NULL
lb$iter <- c(1:length(output$L[-1]))
lb$number_clusters <- output$Cl[-1]

## Plot clusters
plot_lc <- ggplot(lb, aes(x=iter,y=ELBO)) + geom_line(linetype = "dashed") + geom_point()

## Plot number of non-empty clusters
plot_nc <- ggplot(lb, aes(x=iter,y=number_clusters)) + geom_line(linetype = "dashed") + geom_point()

grid.arrange(plot_lb, plot_nc, ncol = 2)
```

Like before, there is no guarantee that the algorithm will converge to the global optimum. However, we observe in practice that this is usually the case.

```{r example1-ind-figure10-7, fig.show='hold', message=FALSE, warning = FALSE, cache=TRUE}
maxK <- 10
n_random_starts <- 100
ELBO <- matrix(0, maxK-1, n_random_starts)
for(k in 2:maxK){
    for(j in 1:n_random_starts){
        output <- vimix(data, K = k, indep = T)
        ELBO[k-1,j] <- output$L[length(output$L)]
    }
}

library(reshape)
ELBO <- melt(t(ELBO))
names(ELBO) <- c('start_n', 'K', 'ELBO')
ELBO$K <- ELBO$K + 1

ggplot(ELBO, aes(x = K, y = ELBO)) + geom_point() + geom_jitter()
```

## Variable selection 

In the case of independent variables, one can also choose to do variable selection and only retain the variables that help define the clustering. 

```{r example1-vs, fig.show = 'hold', message = FALSE, warning = FALSE, cache = TRUE}
## Use variational inference for mixture of Gaussians to find clusters
output <- vimix(data, K = 7, select = T, verbose = T)
```

```{r example1-vs-param, fig.show='hold', message = FALSE, warning = FALSE, cache = TRUE}
library(DirichletReg)

### First variable

pi <- rdirichlet(200, output$model$alpha)
cluster <- lambda <- mean <- obs <- matrix(200, 2)
for(i in 1:200){
    cluster[i] <- which(rmultinom(1, size = 1, prob = pi[i,])!=0)
    lambda[i] <- rgamma(1, shape = output$model$v[1,cluster[i]], rate = 1/(2 * output$model$W[1,cluster[i]]))
    mean[i] <- rnorm(1, output$model$m[1,cluster[i]], 1/(output$model$beta[1,cluster[i]]*lambda[i]))
    obs[i] <- rnorm(1, mean[i], sqrt(lambda[i]))
}
samples <- c(rnorm(200, mean = output$null$m[1], sd = output$null$stdev[1]), obs)
samples <- tidy(samples)
samples$cluster <- as.factor(c(rep('null',200), cluster))
    
ggplot(samples, aes(x = x, fill = cluster, color = cluster)) + 
    geom_density(alpha = 0.1) + xlim(-10,10)

## Second variable

pi <- rdirichlet(200, output$model$alpha)
cluster <- lambda <- mean <- obs <- matrix(200, 2)
for(i in 1:200){
    cluster[i] <- which(rmultinom(1, size = 1, prob = pi[i,])!=0)
    lambda[i] <- rgamma(1, shape = output$model$v[2,cluster[i]], rate = 1/(2 * output$model$W[2,cluster[i]]))
    mean[i] <- rnorm(1, output$model$m[2,cluster[i]], 1/(output$model$beta[2,cluster[i]]*lambda[i]))
    obs[i] <- rnorm(1, mean[i], sqrt(lambda[i]))
}
samples <- c(rnorm(200, mean = output$null$m[1], sd = output$null$stdev[1]), obs)
samples <- tidy(samples)
samples$cluster <- as.factor(c(rep('null',200), cluster))
    
ggplot(samples, aes(x = x, fill = cluster, color = cluster)) + 
    geom_density(alpha = 0.1) + xlim(-10,10)
```

Let's see what our clusters look like:

```{r example1-vs-plot, fig.show='hold', message=FALSE, warning=FALSE, cache=TRUE}
## Plot cluster labels
library(ggplot2)
library(broom)
library(gridExtra)

## Convert data matrix and cluster labels to data.frame
df <- tidy(data)
df$label <- as.factor(output$label)

## Plot clusters
ggplot(df, aes(X1, X2, col = label)) + geom_point()
```

The lower bound increases at each iteration of the algorithm and can be used to check whether the algorithm has converged. Another interesting value to check at each iteration is the number of non-empty clusters.

```{r example1-vs-lb, fig.show='hold', message=FALSE, warning=FALSE, cache=TRUE}
## Check that the lower bound is monotonically increasing

lb <- tidy(output$L[-1])
lb$ELBO <- lb$x
lb$x <- NULL
lb$iter <- c(1:length(output$L[-1]))
lb$number_clusters <- output$Cl[-1]

## Plot lower bound
plot_lb <- ggplot(lb, aes(x=iter,y=ELBO)) + geom_line(linetype = "dashed") + geom_point()

## Plot number of non-empty clusters
plot_nc <- ggplot(lb, aes(x=iter,y=number_clusters)) + geom_line(linetype = "dashed") + geom_point()

grid.arrange(plot_lb, plot_nc, ncol = 2)
```

Finally, it is important to note that the algorithm doesn't always converge to the same solution. Depending on the initialization, it will converge to different local optima. However, by runnin the algorithm multiple times, we can see that, the optimal solution is almost always reached, in this simple case. 

```{r example1-vs-figure10-7, fig.show='hold', message=FALSE, warning = FALSE, cache=TRUE}
maxK <- 4
n_random_starts <- 100
ELBO <- matrix(0, maxK-1, n_random_starts)
for(k in 2:maxK){
    for(j in 1:n_random_starts){
        output <- vimix(data, K = k, select = TRUE)
        ELBO[k-1,j] <- output$L[length(output$L)]
    }
}

library(reshape)
ELBO <- melt(t(ELBO))
names(ELBO) <- c('start_n', 'K', 'ELBO')
ELBO$K <- ELBO$K + 1

ggplot(ELBO, aes(x = K, y = ELBO)) + geom_point() + geom_jitter()
```
